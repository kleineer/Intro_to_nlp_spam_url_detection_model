{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Intro_to_nlp_spam_url_detection_model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Your code here"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "import logging\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import seaborn as sns\n",
                "import matplotlib.pyplot as plt\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.naive_bayes import GaussianNB\n",
                "from sklearn.model_selection import GridSearchCV\n",
                "from sklearn.model_selection import ( train_test_split, GridSearchCV )\n",
                "from sklearn.pipeline import Pipeline\n",
                "from sklearn.preprocessing import ( MinMaxScaler, LabelEncoder, StandardScaler, OneHotEncoder )\n",
                "from sklearn.compose import ColumnTransformer\n",
                "from sklearn.metrics import mean_squared_error, r2_score\n",
                "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
                "from sklearn.feature_selection import SelectKBest, f_regression\n",
                "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
                "from sklearn.metrics import (\n",
                "        accuracy_score,\n",
                "        f1_score,\n",
                "        matthews_corrcoef,\n",
                "        classification_report,\n",
                "        ConfusionMatrixDisplay )\n",
                "from sklearn.inspection import permutation_importance\n",
                "from sklearn.impute import SimpleImputer\n",
                "logger = logging.getLogger()\n",
                "logger.setLevel(logging.INFO)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "[nltk_data] Downloading package stopwords to /home/vscode/nltk_data...\n",
                        "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
                        "[nltk_data] Downloading package wordnet to /home/vscode/nltk_data...\n"
                    ]
                }
            ],
            "source": [
                "import re\n",
                "from nltk.corpus import stopwords\n",
                "from nltk.stem import WordNetLemmatizer\n",
                "from sklearn.model_selection import train_test_split, GridSearchCV\n",
                "from sklearn.svm import SVC\n",
                "from sklearn.feature_extraction.text import TfidfVectorizer\n",
                "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, roc_auc_score, roc_curve\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import pickle\n",
                "from urllib.parse import urlparse\n",
                "from collections import Counter\n",
                "from imblearn.over_sampling import SMOTE  # Para lidiar con el desbalance de clases\n",
                "import nltk\n",
                "nltk.download('stopwords')\n",
                "nltk.download('wordnet')\n",
                "from nltk.tokenize import word_tokenize"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### **1. Data loading**\n",
                "**Objective**: **Obtain the data from source and get a first glimpse of their properties and presentation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 25,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "Información del dataset original:\n",
                        "<class 'pandas.core.frame.DataFrame'>\n",
                        "RangeIndex: 2999 entries, 0 to 2998\n",
                        "Data columns (total 2 columns):\n",
                        " #   Column   Non-Null Count  Dtype \n",
                        "---  ------   --------------  ----- \n",
                        " 0   url      2999 non-null   object\n",
                        " 1   is_spam  2999 non-null   bool  \n",
                        "dtypes: bool(1), object(1)\n",
                        "memory usage: 26.5+ KB\n"
                    ]
                }
            ],
            "source": [
                "#Imprimir la información del dataset\n",
                "url = ('https://raw.githubusercontent.com/4GeeksAcademy/NLP-project-tutorial/main/url_spam.csv')\n",
                "df_raw = pd.read_csv(url, sep=',')\n",
                "print(\"\\nInformación del dataset original:\")\n",
                "df_raw.info()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 26,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Primeras filas del dataset original:\n",
                        "                                                 url  is_spam\n",
                        "0  https://briefingday.us8.list-manage.com/unsubs...     True\n",
                        "1                             https://www.hvper.com/     True\n",
                        "2                 https://briefingday.com/m/v4n3i4f3     True\n",
                        "3   https://briefingday.com/n/20200618/m#commentform    False\n",
                        "4                        https://briefingday.com/fan     True\n"
                    ]
                }
            ],
            "source": [
                "# Imprimir las primeras filas y la información del dataset para familiarizarse con los datos.\n",
                "print(\"Primeras filas del dataset original:\")\n",
                "print(df_raw.head())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### **2. Data preprocessing**\n",
                "**Objectives**: Perform the data cleaning, data transformation and data reduction steps to avoid data mistmatching, noisy data or data not wrangled"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 27,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Paso 2: Preprocesar los enlaces\n",
                "# -----------------------------\n",
                "# Se crea una copia del DataFrame para realizar el preprocesamiento sin modificar los datos originales.\n",
                "df_baking = df_raw.copy()\n",
                "\n",
                "# 2.1 Función para extraer características de la URL\n",
                "def extract_url_features(url):\n",
                "    \"\"\"\n",
                "    Extrae varias características de una URL.\n",
                "\n",
                "    Args:\n",
                "        url (str): La URL a procesar.\n",
                "\n",
                "    Returns:\n",
                "        dict: Un diccionario que contiene las características extraídas.\n",
                "    \"\"\"\n",
                "    parsed_url = urlparse(url)\n",
                "    features = {\n",
                "        'longitud_url': len(url),\n",
                "        'longitud_dominio': len(parsed_url.netloc),\n",
                "        'cantidad_puntos': url.count('.'),\n",
                "        'cantidad_guiones': url.count('-'),\n",
                "        'cantidad_subdominios': parsed_url.netloc.count('.') + 1, # Cuenta el dominio principal como subdominio\n",
                "        'tiene_http': 1 if 'http' in parsed_url.scheme else 0,\n",
                "        'tiene_www': 1 if 'www' in parsed_url.netloc else 0,\n",
                "        'es_ip': 1 if re.match(r'\\d+\\.\\d+\\.\\d+\\.\\d+', parsed_url.netloc) else 0,\n",
                "        'tiene_parametros': 1 if parsed_url.query else 0,\n",
                "        'tiene_fragmento': 1 if parsed_url.fragment else 0\n",
                "    }\n",
                "    return features"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 28,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 2.2 Función para tokenizar y limpiar la URL\n",
                "def tokenize_and_clean_url(url):\n",
                "    \"\"\"\n",
                "    Tokeniza una URL, elimina stopwords y lematiza las palabras.\n",
                "\n",
                "    Args:\n",
                "        url (str): La URL a procesar.\n",
                "\n",
                "    Returns:\n",
                "        str: La URL procesada como una cadena de texto.\n",
                "    \"\"\"\n",
                "    # Eliminar el esquema (http://, https://)\n",
                "    url = re.sub(r'^(https?://)?', '', url)\n",
                "    # Eliminar www.\n",
                "    url = re.sub(r'www\\.', '', url)\n",
                "    # Tokenizar la URL por delimitadores comunes\n",
                "    tokens = re.split(r'[./\\-?=&_]+', url)\n",
                "    # Convertir a minúsculas\n",
                "    tokens = [token.lower() for token in tokens]\n",
                "    # Eliminar stopwords\n",
                "    stop_words = set(stopwords.words('english'))\n",
                "    tokens = [token for token in tokens if token not in stop_words and token]  # Añadido 'token' para evitar cadenas vacías\n",
                "    # Lematizar las palabras\n",
                "    lemmatizer = WordNetLemmatizer()\n",
                "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
                "    return ' '.join(tokens)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 29,
            "metadata": {},
            "outputs": [],
            "source": [
                "def extract_url_features(url):\n",
                "    features = {}\n",
                "    try:\n",
                "        from urllib.parse import urlparse\n",
                "        parsed_url = urlparse(url)\n",
                "        features['netloc'] = parsed_url.netloc\n",
                "        features['path'] = parsed_url.path\n",
                "        features['query'] = parsed_url.query\n",
                "        features['fragment'] = parsed_url.fragment\n",
                "        features['scheme'] = parsed_url.scheme\n",
                "    except:\n",
                "        features['netloc'] = None\n",
                "        features['path'] = None\n",
                "        features['query'] = None\n",
                "        features['fragment'] = None\n",
                "        features['scheme'] = None\n",
                "    return features"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 30,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "['/home/vscode/nltk_data', '/workspaces/Intro_to_nlp_spam_url_detection_model/.venv/nltk_data', '/workspaces/Intro_to_nlp_spam_url_detection_model/.venv/share/nltk_data', '/workspaces/Intro_to_nlp_spam_url_detection_model/.venv/lib/nltk_data', '/usr/share/nltk_data', '/usr/local/share/nltk_data', '/usr/lib/nltk_data', '/usr/local/lib/nltk_data']\n"
                    ]
                }
            ],
            "source": [
                "print(nltk.data.path)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 31,
            "metadata": {},
            "outputs": [],
            "source": [
                "def tokenize_and_clean_url(url):\n",
                "    try:\n",
                "        # Convertir a minúsculas\n",
                "        url = url.lower()\n",
                "        # Eliminar caracteres especiales (manteniendo letras, números y algunos símbolos de URL)\n",
                "        url = re.sub(r'[^a-zA-Z0-9./?=&#-]', ' ', url)\n",
                "        # Tokenizar la URL\n",
                "        tokens = word_tokenize(url)\n",
                "        # Obtener las stopwords en inglés (puedes cambiar el idioma si tus URLs son en otro idioma)\n",
                "        stop_words = set(stopwords.words('english'))\n",
                "        # Eliminar stopwords y tokens vacíos\n",
                "        cleaned_tokens = [w for w in tokens if not w in stop_words and len(w) > 0]\n",
                "        return \" \".join(cleaned_tokens)\n",
                "    except:\n",
                "        return \"\"\n",
                "\n",
                "# Descargar el recurso de stopwords de NLTK (ESTA ES LA LÍNEA AÑADIDA)\n",
                "try:\n",
                "    stopwords.words('english') # Intenta acceder a las stopwords para verificar si ya están descargadas\n",
                "except LookupError:\n",
                "    nltk.download('stopwords')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### **3. Exploratory Data Analysis** \n",
                "**Objective**: Summarize the main characteristics of the dataset using descriptive statistics and data visualization methods"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 32,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "Primeras filas del dataset preprocesado (df_baking):\n",
                        "                                                 url  is_spam  \\\n",
                        "0  https://briefingday.us8.list-manage.com/unsubs...     True   \n",
                        "1                             https://www.hvper.com/     True   \n",
                        "2                 https://briefingday.com/m/v4n3i4f3     True   \n",
                        "3   https://briefingday.com/n/20200618/m#commentform    False   \n",
                        "4                        https://briefingday.com/fan     True   \n",
                        "\n",
                        "                                        url_features  \\\n",
                        "0  {'netloc': 'briefingday.us8.list-manage.com', ...   \n",
                        "1  {'netloc': 'www.hvper.com', 'path': '/', 'quer...   \n",
                        "2  {'netloc': 'briefingday.com', 'path': '/m/v4n3...   \n",
                        "3  {'netloc': 'briefingday.com', 'path': '/n/2020...   \n",
                        "4  {'netloc': 'briefingday.com', 'path': '/fan', ...   \n",
                        "\n",
                        "                            netloc           path query     fragment scheme  \\\n",
                        "0  briefingday.us8.list-manage.com   /unsubscribe                     https   \n",
                        "1                    www.hvper.com              /                     https   \n",
                        "2                  briefingday.com    /m/v4n3i4f3                     https   \n",
                        "3                  briefingday.com  /n/20200618/m        commentform  https   \n",
                        "4                  briefingday.com           /fan                     https   \n",
                        "\n",
                        "  cleaned_url  \n",
                        "0              \n",
                        "1              \n",
                        "2              \n",
                        "3              \n",
                        "4              \n",
                        "\n",
                        "Información del dataset preprocesado (df_baking):\n",
                        "<class 'pandas.core.frame.DataFrame'>\n",
                        "RangeIndex: 2999 entries, 0 to 2998\n",
                        "Data columns (total 9 columns):\n",
                        " #   Column        Non-Null Count  Dtype \n",
                        "---  ------        --------------  ----- \n",
                        " 0   url           2999 non-null   object\n",
                        " 1   is_spam       2999 non-null   bool  \n",
                        " 2   url_features  2999 non-null   object\n",
                        " 3   netloc        2999 non-null   object\n",
                        " 4   path          2999 non-null   object\n",
                        " 5   query         2999 non-null   object\n",
                        " 6   fragment      2999 non-null   object\n",
                        " 7   scheme        2999 non-null   object\n",
                        " 8   cleaned_url   2999 non-null   object\n",
                        "dtypes: bool(1), object(8)\n",
                        "memory usage: 190.5+ KB\n",
                        "None\n"
                    ]
                }
            ],
            "source": [
                "# 2.3 Aplicar las funciones de extracción y limpieza\n",
                "df_baking['url_features'] = df_baking['url'].apply(extract_url_features)\n",
                "\n",
                "# Crear columnas separadas para cada característica extraída\n",
                "df_features = df_baking['url_features'].apply(pd.Series)\n",
                "df_baking = pd.concat([df_baking, df_features], axis=1)\n",
                "\n",
                "df_baking['cleaned_url'] = df_baking['url'].apply(tokenize_and_clean_url)\n",
                "\n",
                "# Imprimir las primeras filas y la información del dataset preprocesado.\n",
                "print(\"\\nPrimeras filas del dataset preprocesado (df_baking):\")\n",
                "print(df_baking.head())\n",
                "print(\"\\nInformación del dataset preprocesado (df_baking):\")\n",
                "print(df_baking.info())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 33,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "<class 'pandas.core.frame.DataFrame'>\n",
                        "RangeIndex: 2999 entries, 0 to 2998\n",
                        "Data columns (total 9 columns):\n",
                        " #   Column        Non-Null Count  Dtype \n",
                        "---  ------        --------------  ----- \n",
                        " 0   url           2999 non-null   object\n",
                        " 1   is_spam       2999 non-null   bool  \n",
                        " 2   url_features  2999 non-null   object\n",
                        " 3   netloc        2999 non-null   object\n",
                        " 4   path          2999 non-null   object\n",
                        " 5   query         2999 non-null   object\n",
                        " 6   fragment      2999 non-null   object\n",
                        " 7   scheme        2999 non-null   object\n",
                        " 8   cleaned_url   2999 non-null   object\n",
                        "dtypes: bool(1), object(8)\n",
                        "memory usage: 190.5+ KB\n"
                    ]
                }
            ],
            "source": [
                "df_baking.info()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 24,
            "metadata": {},
            "outputs": [
                {
                    "ename": "ValueError",
                    "evalue": "Could not interpret value `longitud_url` for `y`. An entry with this name does not appear in `data`.",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
                        "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
                        "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# 2.4 Análisis Bivariado (Ejemplo: Longitud de la URL vs. is_spam)\u001b[39;00m\n\u001b[32m      2\u001b[39m plt.figure(figsize=(\u001b[32m10\u001b[39m, \u001b[32m6\u001b[39m))\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43msns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mboxplot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mis_spam\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlongitud_url\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdf_baking\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m plt.title(\u001b[33m'\u001b[39m\u001b[33mAnálisis Bivariado: Longitud de la URL vs. is_spam\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      5\u001b[39m plt.xlabel(\u001b[33m'\u001b[39m\u001b[33m¿Es Spam?\u001b[39m\u001b[33m'\u001b[39m)\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/Intro_to_nlp_spam_url_detection_model/.venv/lib/python3.11/site-packages/seaborn/categorical.py:1597\u001b[39m, in \u001b[36mboxplot\u001b[39m\u001b[34m(data, x, y, hue, order, hue_order, orient, color, palette, saturation, fill, dodge, width, gap, whis, linecolor, linewidth, fliersize, hue_norm, native_scale, log_scale, formatter, legend, ax, **kwargs)\u001b[39m\n\u001b[32m   1589\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mboxplot\u001b[39m(\n\u001b[32m   1590\u001b[39m     data=\u001b[38;5;28;01mNone\u001b[39;00m, *, x=\u001b[38;5;28;01mNone\u001b[39;00m, y=\u001b[38;5;28;01mNone\u001b[39;00m, hue=\u001b[38;5;28;01mNone\u001b[39;00m, order=\u001b[38;5;28;01mNone\u001b[39;00m, hue_order=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1591\u001b[39m     orient=\u001b[38;5;28;01mNone\u001b[39;00m, color=\u001b[38;5;28;01mNone\u001b[39;00m, palette=\u001b[38;5;28;01mNone\u001b[39;00m, saturation=\u001b[32m.75\u001b[39m, fill=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1594\u001b[39m     legend=\u001b[33m\"\u001b[39m\u001b[33mauto\u001b[39m\u001b[33m\"\u001b[39m, ax=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs\n\u001b[32m   1595\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1597\u001b[39m     p = \u001b[43m_CategoricalPlotter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1598\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1599\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvariables\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m=\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m=\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhue\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhue\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1600\u001b[39m \u001b[43m        \u001b[49m\u001b[43morder\u001b[49m\u001b[43m=\u001b[49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1601\u001b[39m \u001b[43m        \u001b[49m\u001b[43morient\u001b[49m\u001b[43m=\u001b[49m\u001b[43morient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1602\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcolor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1603\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlegend\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlegend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1604\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1606\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ax \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1607\u001b[39m         ax = plt.gca()\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/Intro_to_nlp_spam_url_detection_model/.venv/lib/python3.11/site-packages/seaborn/categorical.py:67\u001b[39m, in \u001b[36m_CategoricalPlotter.__init__\u001b[39m\u001b[34m(self, data, variables, order, orient, require_numeric, color, legend)\u001b[39m\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\n\u001b[32m     57\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m     58\u001b[39m     data=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     64\u001b[39m     legend=\u001b[33m\"\u001b[39m\u001b[33mauto\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     65\u001b[39m ):\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariables\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvariables\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     69\u001b[39m     \u001b[38;5;66;03m# This method takes care of some bookkeeping that is necessary because the\u001b[39;00m\n\u001b[32m     70\u001b[39m     \u001b[38;5;66;03m# original categorical plots (prior to the 2021 refactor) had some rules that\u001b[39;00m\n\u001b[32m     71\u001b[39m     \u001b[38;5;66;03m# don't fit exactly into VectorPlotter logic. It may be wise to have a second\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     76\u001b[39m     \u001b[38;5;66;03m# default VectorPlotter rules. If we do decide to make orient part of the\u001b[39;00m\n\u001b[32m     77\u001b[39m     \u001b[38;5;66;03m# _base variable assignment, we'll want to figure out how to express that.\u001b[39;00m\n\u001b[32m     78\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.input_format == \u001b[33m\"\u001b[39m\u001b[33mwide\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m orient \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33mh\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33my\u001b[39m\u001b[33m\"\u001b[39m]:\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/Intro_to_nlp_spam_url_detection_model/.venv/lib/python3.11/site-packages/seaborn/_base.py:634\u001b[39m, in \u001b[36mVectorPlotter.__init__\u001b[39m\u001b[34m(self, data, variables)\u001b[39m\n\u001b[32m    629\u001b[39m \u001b[38;5;66;03m# var_ordered is relevant only for categorical axis variables, and may\u001b[39;00m\n\u001b[32m    630\u001b[39m \u001b[38;5;66;03m# be better handled by an internal axis information object that tracks\u001b[39;00m\n\u001b[32m    631\u001b[39m \u001b[38;5;66;03m# such information and is set up by the scale_* methods. The analogous\u001b[39;00m\n\u001b[32m    632\u001b[39m \u001b[38;5;66;03m# information for numeric axes would be information about log scales.\u001b[39;00m\n\u001b[32m    633\u001b[39m \u001b[38;5;28mself\u001b[39m._var_ordered = {\u001b[33m\"\u001b[39m\u001b[33mx\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33my\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m}  \u001b[38;5;66;03m# alt., used DefaultDict\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m634\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43massign_variables\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariables\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    636\u001b[39m \u001b[38;5;66;03m# TODO Lots of tests assume that these are called to initialize the\u001b[39;00m\n\u001b[32m    637\u001b[39m \u001b[38;5;66;03m# mappings to default values on class initialization. I'd prefer to\u001b[39;00m\n\u001b[32m    638\u001b[39m \u001b[38;5;66;03m# move away from that and only have a mapping when explicitly called.\u001b[39;00m\n\u001b[32m    639\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m var \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33mhue\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33msize\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstyle\u001b[39m\u001b[33m\"\u001b[39m]:\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/Intro_to_nlp_spam_url_detection_model/.venv/lib/python3.11/site-packages/seaborn/_base.py:679\u001b[39m, in \u001b[36mVectorPlotter.assign_variables\u001b[39m\u001b[34m(self, data, variables)\u001b[39m\n\u001b[32m    674\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    675\u001b[39m     \u001b[38;5;66;03m# When dealing with long-form input, use the newer PlotData\u001b[39;00m\n\u001b[32m    676\u001b[39m     \u001b[38;5;66;03m# object (internal but introduced for the objects interface)\u001b[39;00m\n\u001b[32m    677\u001b[39m     \u001b[38;5;66;03m# to centralize / standardize data consumption logic.\u001b[39;00m\n\u001b[32m    678\u001b[39m     \u001b[38;5;28mself\u001b[39m.input_format = \u001b[33m\"\u001b[39m\u001b[33mlong\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m679\u001b[39m     plot_data = \u001b[43mPlotData\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariables\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    680\u001b[39m     frame = plot_data.frame\n\u001b[32m    681\u001b[39m     names = plot_data.names\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/Intro_to_nlp_spam_url_detection_model/.venv/lib/python3.11/site-packages/seaborn/_core/data.py:58\u001b[39m, in \u001b[36mPlotData.__init__\u001b[39m\u001b[34m(self, data, variables)\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\n\u001b[32m     52\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m     53\u001b[39m     data: DataSource,\n\u001b[32m     54\u001b[39m     variables: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, VariableSpec],\n\u001b[32m     55\u001b[39m ):\n\u001b[32m     57\u001b[39m     data = handle_data_source(data)\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m     frame, names, ids = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_assign_variables\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariables\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     60\u001b[39m     \u001b[38;5;28mself\u001b[39m.frame = frame\n\u001b[32m     61\u001b[39m     \u001b[38;5;28mself\u001b[39m.names = names\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/Intro_to_nlp_spam_url_detection_model/.venv/lib/python3.11/site-packages/seaborn/_core/data.py:232\u001b[39m, in \u001b[36mPlotData._assign_variables\u001b[39m\u001b[34m(self, data, variables)\u001b[39m\n\u001b[32m    230\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    231\u001b[39m         err += \u001b[33m\"\u001b[39m\u001b[33mAn entry with this name does not appear in `data`.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m232\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(err)\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    235\u001b[39m \n\u001b[32m    236\u001b[39m     \u001b[38;5;66;03m# Otherwise, assume the value somehow represents data\u001b[39;00m\n\u001b[32m    237\u001b[39m \n\u001b[32m    238\u001b[39m     \u001b[38;5;66;03m# Ignore empty data structures\u001b[39;00m\n\u001b[32m    239\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(val, Sized) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(val) == \u001b[32m0\u001b[39m:\n",
                        "\u001b[31mValueError\u001b[39m: Could not interpret value `longitud_url` for `y`. An entry with this name does not appear in `data`."
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "<Figure size 1000x600 with 0 Axes>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "# 2.4 Análisis Bivariado (Ejemplo: Longitud de la URL vs. is_spam)\n",
                "plt.figure(figsize=(10, 6))\n",
                "sns.boxplot(x='is_spam', y='longitud_url', data=df_baking)\n",
                "plt.title('Análisis Bivariado: Longitud de la URL vs. is_spam')\n",
                "plt.xlabel('¿Es Spam?')\n",
                "plt.ylabel('Longitud de la URL')\n",
                "plt.show()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.4"
        },
        "orig_nbformat": 4
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
